# -*- coding: utf-8 -*-
"""wine_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i-ji4IRLbFaBZjh330maDOZbL1aK-NI8

# Wine data - Clustering

##Goal: 
Discover the optimal number of customer segments to focus on based on the features of various types of wine. (Note: the dataset already comes with a variable named Customer_Segment which has 3 segments/clusters. This is useful for building a model which can be applied to the segmentation of future customers.)

 
###Part 1: K-Mean Clustering

Uses the KMeans() class from the sklearn.cluster module.  

Finds the optimal number of clusters using both the Elbow method and the Silhouette method.

###Part 2:  Hierarchical Clustering

Uses the AgglomerativeClustering() class from sklearn.cluster module.  

Plots the dendrogram. Identification of the optimal number of clusters.

###Part 3: Model Comparison

Uses the rand_score function from sklearn.metrics to compare the performances of the clustering models with respect to the true customer segments.
"""

from pandas.core.common import random_state
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

rand_state = 1000

#Read in the data
df = pd.read_csv("https://raw.githubusercontent.com/PJalgotrader/Machine_Learning-USU/fall22/data/Wine.csv")
df.head()

"""Customer_Segment variable was dropped for analysis. The variable will be used to score the models at the end of the assessment."""

X = df.drop('Customer_Segment', axis=1, inplace =False)

"""# **Part 1**"""

#Scale the Data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
df_sc = sc.fit_transform(df)

WCSS=[]
silhouette_scores = [] 
K= 15
for i in range(2,K+1): 
    kmeans= KMeans(n_clusters=i, random_state=100)
    kmeans.fit(df_sc)
    WCSS.append(kmeans.inertia_) 
    scores = silhouette_score(df_sc, kmeans.labels_)
    silhouette_scores.append(scores)
  
optimal_k = pd.DataFrame({'K':range(2,K+1), 'WCSS':WCSS, 'silhouette_score':silhouette_scores})
optimal_k

sns.lineplot(x='K', y='WCSS', data=  optimal_k)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

sns.lineplot(x='K', y='silhouette_score', data=  optimal_k)
plt.title('The Silhouette Method')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.show()

"""From the elbow method and the silhouette score it seems that the optimal number of clusters is k = 3. The elbow method shows that improvement slows dramatically after k = 3. The silhouette method agrees that 3 is the best value for k."""

kmeans_label = KMeans(n_clusters=3, random_state=100).fit_predict(df_sc)

"""# **Part 2**"""

import scipy.cluster.hierarchy as sch

plt.figure(figsize=(16,10))
dend = sch.dendrogram(sch.linkage(df_sc,method='ward'))

"""The optimal number of clusters appears to be three. The height of the lines in the yellow, green, and red sections are short, representing that categories within these groups are differentiable but not very different. The height of the blue dendrites shows how different each of these three groups are."""

from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')
hierarchical_labels = hc.fit_predict(df_sc)

"""# **Part 3**"""

rand_score(kmeans_label, df['Customer_Segment'])

from sklearn.metrics import rand_score
rand_score(hierarchical_labels, df['Customer_Segment'])

"""# Which Model is the Best? 
The k-means classification model has 97% accuracy. This is higher than the hierarchical model's 95% accuracy score.

Each of these models might be improved with the use of additional methods which can be used within or alongside each classification model, but the simple models shown here are compared as provided.

"""